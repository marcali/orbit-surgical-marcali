seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/develop/modules/skrl.utils.model_instantiators.html
models:
  separate: true
  policy:
    clip_actions: False
    clip_log_std: True
    initial_log_std: 0
    min_log_std: -20.0
    max_log_std: 2.0
    input_shape: "Shape.STATES"
    hiddens: [512, 256, 128, 64]
    hidden_activation: ["elu","elu", "elu", "elu"]
    output_shape: "Shape.ACTIONS"
    output_activation: ""
    output_scale: 1.0

  critic_1:
    clip_actions: False
    input_shape: "Shape.STATES_ACTIONS"
    hiddens: [256, 128]
    hidden_activation: ["elu", "elu"]
    output_shape: "Shape.ONE"  # Enclose 1 in quotes
    output_activation: ""
    output_scale: 1.0

  critic_2:
    # Same as critic_1
    clip_actions: False
    input_shape: "Shape.STATES_ACTIONS"
    hiddens: [256, 128]
    hidden_activation: ["elu", "elu"]
    output_shape: "Shape.ONE"  # Enclose 1 in quotes
    output_activation: ""
    output_scale: 1.0

  target_critic_1:
    # Same as critic_1
    clip_actions: False
    input_shape: "Shape.STATES_ACTIONS"
    hiddens: [256, 128]
    hidden_activation: ["elu", "elu"]
    output_shape: "Shape.ONE"  # Enclose 1 in quotes
    output_activation: ""
    output_scale: 1.0

  target_critic_2:
    # Same as critic_1
    clip_actions: False
    input_shape: "Shape.STATES_ACTIONS"
    hiddens: [256, 128]
    hidden_activation: ["elu", "elu"]
    output_shape: "Shape.ONE"  # Enclose 1 in quotes
    output_activation: ""
    output_scale: 1.0
  
agent:
  gradient_steps: 1
  batch_size: 64
  discount_factor: 0.98
  polyak: 0.01   # For target networks in SAC (also known as tau)
  actor_learning_rate: 1.e-4
  critic_learning_rate: 1.e-4
  automatic_entropy_tuning: True
  random_timesteps: 0
  learning_starts: 1000
  state_preprocessor: "RunningStandardScaler"
  state_preprocessor_kwargs: null
  value_preprocessor: "RunningStandardScaler"
  value_preprocessor_kwargs: null

  grad_norm_clip: 0.5
  learn_entropy: True
  entropy_learning_rate: 1.e-4
  initial_entropy_value: 0.2
  rewards_shaper_scale: 0.01
  # logging and checkpoint
  experiment:
    directory: "sac_lift"
    experiment_name: ""
    write_interval: 120
    checkpoint_interval: 1200



# Sequential trainer
# https://skrl.readthedocs.io/en/latest/modules/skrl.trainers.sequential.html
trainer:
  timesteps: 50000
  environment_info: "log"