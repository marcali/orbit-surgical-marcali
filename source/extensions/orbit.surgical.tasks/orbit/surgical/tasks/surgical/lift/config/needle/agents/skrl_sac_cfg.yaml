seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/develop/modules/skrl.utils.model_instantiators.html
models:
  separate: true
  policy:  # see skrl.utils.model_instantiators.gaussian_model for parameter details
    clip_actions: False
    clip_log_std: True
    initial_log_std: 0
    min_log_std: -20.0
    max_log_std: 2.0
    input_shape: "Shape.STATES"
    hiddens: [256, 128, 64]
    hidden_activation: ["elu", "elu", "elu"]
    output_shape: "Shape.ACTIONS"
    output_activation: ""
    output_scale: 1.0
  critic_1:
    clip_actions: False
    input_shape: "Shape.STATES"
    hiddens: [512, 256]
    hidden_activation: ['relu', 'relu']
    output_shape: "Shape.ACTIONS"
    output_activation: "tanh"
    output_scale: 1.0
  critic_2:
    clip_actions: False
    input_shape: "Shape.STATES"
    hiddens: [512, 256]
    hidden_activation: ['relu', 'relu']
    output_shape: "Shape.ACTIONS"
    output_activation: "tanh"
    output_scale: 1.0
  target_critic_1:
    clip_actions: False
    input_shape: "Shape.STATES"
    hiddens: [512, 256]
    hidden_activation: ['relu', 'relu']
    output_shape: "Shape.ACTIONS"
    output_activation: "tanh"
    output_scale: 1.0
  target_critic_2:
    clip_actions: False
    input_shape: "Shape.STATES"
    hiddens: [512, 256]
    hidden_activation: ['relu', 'relu']
    output_shape: "Shape.ACTIONS"
    output_activation: "tanh"
    output_scale: 1.0
  
agent:
  gradient_steps: 1
  batch_size: 64
  discount_factor: 0.98
  soft_update_rate: 0.005   # For target networks in SAC (also known as tau)
  learning_rate: 3.e-4       # Common learning rate for both actor and critic
  actor_learning_rate: 3.e-4
  critic_learning_rate: 3.e-4
  alpha_learning_rate: 3.e-4 # If temperature is learnable
  initial_alpha: 0.2        # Initial value for entropy coefficient (alpha)
  automatic_entropy_tuning: True
  random_timesteps: 0
  learning_starts: 1000
  replay_buffer_size: 1000000
  grad_norm_clip: 1.0
  state_preprocessor: "RunningStandardScaler"
  value_preprocessor: "RunningStandardScaler"
  rewards_shaper_scale: 0.01
  # logging and checkpoint
  experiment:
    directory: "sac_lift"
    experiment_name: ""
    write_interval: 120
    checkpoint_interval: 1200



# Sequential trainer
# https://skrl.readthedocs.io/en/latest/modules/skrl.trainers.sequential.html
trainer:
  timesteps: 24000
  environment_info: "log"